{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d6f78500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9640826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.test.utils\n",
    "\n",
    "# Set file names for train and test data\n",
    "lee_train_file = gensim.test.utils.datapath('lee_background.cor')\n",
    "lee_test_file = gensim.test.utils.datapath('lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1f1d0632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['the', 'national', 'road', 'toll', 'for', 'the', 'christmas', 'new', 'year', 'holiday', 'period', 'stands', 'at', 'eight', 'fewer', 'than', 'for', 'the', 'same', 'time', 'last', 'year', 'people', 'have', 'died', 'on', 'new', 'south', 'wales', 'roads', 'with', 'eight', 'fatalities', 'in', 'both', 'queensland', 'and', 'victoria', 'western', 'australia', 'the', 'northern', 'territory', 'and', 'south', 'australia', 'have', 'each', 'recorded', 'three', 'deaths', 'while', 'the', 'act', 'and', 'tasmania', 'remain', 'fatality', 'free'], [2]>\n",
      "['the', 'united', 'states', 'government', 'has', 'said', 'it', 'wants', 'to', 'see', 'president', 'robert', 'mugabe', 'removed', 'from', 'power', 'and', 'that', 'it', 'is', 'working', 'with', 'the', 'zimbabwean', 'opposition', 'to', 'bring', 'about', 'change', 'of', 'administration', 'as', 'scores', 'of', 'white', 'farmers', 'went', 'into', 'hiding', 'to', 'escape', 'round', 'up', 'by', 'zimbabwean', 'police', 'senior', 'bush', 'administration', 'official', 'called', 'mr', 'mugabe', 'rule', 'illegitimate', 'and', 'irrational', 'and', 'said', 'that', 'his', 're', 'election', 'as', 'president', 'in', 'march', 'was', 'won', 'through', 'fraud', 'walter', 'kansteiner', 'the', 'assistant', 'secretary', 'of', 'state', 'for', 'african', 'affairs', 'went', 'on', 'to', 'blame', 'mr', 'mugabe', 'policies', 'for', 'contributing', 'to', 'the', 'threat', 'of', 'famine', 'in', 'zimbabwe']\n"
     ]
    }
   ],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
    "\n",
    "print(train_corpus[2])\n",
    "print(test_corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6877370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "600f1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "21ea031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d048004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x19c4f173f20>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "41c696d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22893533 -0.20554085 -0.13129798  0.22304149 -0.11186235 -0.09403428\n",
      " -0.00075656  0.03464491 -0.25888056 -0.10293125  0.196114   -0.00196223\n",
      "  0.03390473 -0.11785848 -0.04237121 -0.2999309   0.04176273  0.26153782\n",
      "  0.19902906 -0.13180555  0.0401841  -0.00735625  0.11372375  0.02609809\n",
      " -0.00908577 -0.03928182 -0.24520338 -0.01027686 -0.23355114 -0.0358557\n",
      "  0.36941603 -0.07787085  0.19498177  0.11011455  0.1256837   0.09161688\n",
      " -0.04098217 -0.20442925 -0.06043252  0.0499398  -0.00217425 -0.02434188\n",
      "  0.01746238 -0.06466892  0.0871626   0.07535709 -0.04890525 -0.02570827\n",
      "  0.15973866 -0.02477881]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "87d6c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 292, 1: 8})\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea6d62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (30): ¬´police are combing through videotapes trying to spot the gunman dressed in black who shot year old man to death at downtown massage parlour the victim was hit in the stomach and upper body and died about hours later in hospital the woman was not hurt police urged business owners to turn over any security camera videotapes they might have that recorded people on the street at the time several such videos are now being reviewed¬ª\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
      "\n",
      "MOST (198, 0.7682719230651855): ¬´authorities are trying to track down the crew of vessel that landed undetected at cocos islands carrying asylum seekers the group of sri lankan men was found aboard their boat moored to the south of the islands yesterday afternoon shire president ron grant says investigations are underway as to the whereabouts of the crew after the asylum seekers told authorities they had left in another boat after dropping them off unfortunately for them there two aircraft the royal australian air force here at the moment and one getting prepared to fly off and obviously they will be looking to see if there is another boat he said mr grant says the sri lankans have not yet been brought ashore¬ª\n",
      "\n",
      "MEDIAN (166, 0.3773265779018402): ¬´the federal government says asio and the australian federal police have interviewed the family of an australian man who has been fighting with the taliban in afghanistan the year old man was arrested by the northern alliance at the weekend the federal attorney general daryl williams says he cannot confirm media reports the man has family in adelaide but he says he has had more extensive training than american john walker lindh who was arrested on december for fighting with the taliban he is understood to have travelled to europe in mid to join the kosovo liberation army he then travelled to pakistan november where he undertook some training he entered afghanistan as we understand it in and he has actually undertaken extensive training with osama bin laden al qaeda network mr williams said¬ª\n",
      "\n",
      "LEAST (245, 0.06286976486444473): ¬´federal treasurer peter costello has warned continued economic growth in australia is dependent on an uncertain world outlook the latest figures show the economy grew by per cent in the september quarter mr costello is stressing the seriousness of the current global economic downturn as serious as anything we ve seen in the last two decades he said both he and the reserve bank governor ian macfarlane believe the timing of any pickup is unclear mr costello is hopeful about the united states prospects next year and says pickup sooner rather than later in the year could keep australia in strong position think with every confidence the december quarter is going to be strong if the us comes back we might defy again what has happened in the world he said shadow treasurer bob mcmullan says the treasurer is left relying on things out of his control all the treasurer has got in plan for is the hope the american economy will pick up in time he said he says the government has limited its own ability to respond to the international shocks by spending too much this year¬ª\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): ¬´{}¬ª\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: ¬´%s¬ª\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5065b59",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f311d",
   "metadata": {},
   "source": [
    "### Task 0. Train your own doc2vec model on a test dataset. Most of the example files use Parquet file format. A short guide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "23c540e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21689 entries, 0 to 21688\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   text              21689 non-null  object\n",
      " 1   label             21689 non-null  object\n",
      " 2   tweet_hashtags    21689 non-null  object\n",
      " 3   datetime          21689 non-null  object\n",
      " 4   username_encoded  21689 non-null  object\n",
      " 5   url_encoded       21689 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1016.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df =  pd.read_parquet(\"train-DataEntity_chunk_1.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f940e8",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "abfe2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import pandas as pd\n",
    "\n",
    "# –í–∏–±–∏—Ä–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç–µ–∫—Å—Ç–∏ ‚Äî –Ω–µ –±—ñ–ª—å—à–µ 10000 –∑–∞–ø–∏—Å—ñ–≤ –¥–ª—è —Ç–µ—Å—Ç—É\n",
    "texts = df['text'].dropna().astype(str).sample(10000, random_state=42)\n",
    "\n",
    "# –°—Ç–≤–æ—Ä—é—î–º–æ TaggedDocuments\n",
    "tagged_data = [\n",
    "    TaggedDocument(words=simple_preprocess(text), tags=[str(i)]) \n",
    "    for i, text in enumerate(texts)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5dd19",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0995a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Doc2Vec\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# model = Doc2Vec(\n",
    "#     vector_size=100,\n",
    "#     alpha=0.025,\n",
    "#     min_alpha=0.00025,\n",
    "#     min_count=2,\n",
    "#     dm=1,\n",
    "#     epochs=20,\n",
    "#     workers=4\n",
    "# )\n",
    "\n",
    "# model.build_vocab(tagged_data)\n",
    "\n",
    "# for epoch in tqdm(range(20)):\n",
    "#     model.train(tagged_data, total_examples=model.corpus_count, epochs=1)\n",
    "#     model.alpha -= 0.001\n",
    "#     model.min_alpha = model.alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4213b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [00:00<00:32,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:01<00:30,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 3/50 [00:01<00:30,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 4/50 [00:02<00:30,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 5/50 [00:03<00:33,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 6/50 [00:04<00:32,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 7/50 [00:05<00:33,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 8/50 [00:05<00:33,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 9/50 [00:06<00:30,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 10/50 [00:07<00:28,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 11/50 [00:07<00:26,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 12/50 [00:08<00:25,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 13/50 [00:09<00:24,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 14/50 [00:09<00:22,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 15/50 [00:10<00:21,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [00:10<00:20,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [00:11<00:19,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [00:12<00:19,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [00:12<00:19,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [00:13<00:18,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [00:13<00:17,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [00:14<00:16,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [00:15<00:16,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [00:15<00:15,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [00:16<00:14,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [00:16<00:13,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [00:17<00:13,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [00:17<00:12,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [00:18<00:12,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [00:19<00:11,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [00:19<00:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [00:20<00:10,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [00:20<00:09,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [00:21<00:09,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [00:21<00:08,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [00:22<00:08,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [00:23<00:07,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [00:23<00:06,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [00:24<00:06,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [00:24<00:05,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [00:25<00:05,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [00:26<00:04,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [00:26<00:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [00:27<00:03,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [00:27<00:03,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [00:28<00:02,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [00:29<00:01,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [00:29<00:01,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [00:30<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:31<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Initialize the model with improved parameters\n",
    "model = Doc2Vec(\n",
    "    vector_size=150,  # Increased vector size\n",
    "    alpha=0.025,\n",
    "    min_alpha=0.0001,\n",
    "    min_count=2,\n",
    "    dm=1,  # Distributed Memory\n",
    "    negative=5,  # Negative sampling\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Build vocabulary\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the model with dynamic alpha adjustment and shuffled data\n",
    "for epoch in tqdm(range(50)):  # Increased epochs\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    random.shuffle(tagged_data)  # Shuffle data\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.0005  # Decrease alpha\n",
    "    model.min_alpha = model.alpha  # Update min_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17fadd",
   "metadata": {},
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8faf29c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.6128641e-03  2.1444603e-03 -7.9049566e-04  3.2517393e-04\n",
      "  6.9712085e-04  9.2040934e-04 -9.4193219e-06 -1.5700740e-03\n",
      " -2.2711023e-03  4.7641515e-04  7.9885044e-04 -2.4548757e-03\n",
      " -8.5811061e-04 -2.3597665e-03 -2.0153746e-03  1.1516976e-03\n",
      " -2.2422867e-03 -2.2186693e-03  1.7216094e-03  2.7411906e-03\n",
      " -3.0355859e-03 -1.3933218e-03 -1.9817539e-03 -1.1383190e-03\n",
      "  2.4433406e-03  1.4289221e-03 -1.5106569e-03  4.5542835e-04\n",
      "  5.3954718e-04  1.0516075e-03 -1.5615106e-03 -4.8427124e-04\n",
      " -3.9304851e-04 -1.3317253e-03  1.8481044e-03 -1.3050179e-04\n",
      " -2.2933902e-03  1.7656616e-03 -1.2762546e-05 -3.1037231e-03\n",
      " -2.6975470e-03 -2.7352175e-05 -2.2657705e-03 -1.7478792e-03\n",
      " -1.7523193e-03  1.4055959e-03 -7.3113106e-04 -1.5913765e-03\n",
      " -3.0923299e-03  1.8568508e-03 -1.3822758e-03 -1.8616552e-03\n",
      " -1.5303642e-03 -2.2710608e-03 -2.5493912e-03  2.4251144e-06\n",
      "  3.0446993e-03 -2.9113453e-03 -8.6582761e-04 -3.3022314e-03\n",
      "  4.2387008e-04  2.1469120e-03 -3.1847300e-03 -5.8678130e-04\n",
      " -1.6413848e-03 -2.6728115e-03 -8.5399509e-04  9.4403705e-04\n",
      " -2.0544841e-03  2.2981369e-03 -8.7521672e-05  2.8576693e-04\n",
      "  3.0828062e-03 -2.8880828e-03  3.0008561e-03  2.6104489e-04\n",
      "  1.6584989e-03  2.9983628e-03 -3.1906585e-04  5.5304688e-04\n",
      " -5.9436698e-04 -1.0933201e-03 -3.1908848e-03  3.1866196e-03\n",
      "  1.2357029e-03  1.2760627e-03  2.6406848e-03 -1.0458094e-03\n",
      " -3.2876669e-03 -2.7859604e-03 -7.6797209e-04  1.3942918e-03\n",
      "  1.9884149e-04 -2.0120868e-03  2.9070452e-03  1.2807504e-03\n",
      " -1.8907735e-03 -3.2768650e-03 -2.8070365e-03 -1.0732713e-03\n",
      " -2.5604381e-03  6.1412295e-04 -6.6522596e-04 -6.3367706e-04\n",
      " -1.9922114e-03 -1.0517003e-03 -2.7706642e-03 -2.9087826e-03\n",
      " -4.5787654e-04  6.8849325e-04  1.6837367e-03  3.3153705e-03\n",
      " -2.5201321e-03 -1.7296115e-04 -2.4368202e-03  2.1800338e-03\n",
      "  6.9355167e-04  1.4445182e-03 -3.0498037e-03  2.1848420e-03\n",
      " -2.1521372e-03 -9.5598720e-04  2.7586566e-03 -3.1392202e-03\n",
      " -2.6561221e-04  2.7377286e-03  3.2775449e-03 -9.9810539e-04\n",
      "  1.0172972e-03  1.7104431e-03 -1.9602377e-03  1.0730346e-04\n",
      " -2.2531564e-03 -3.0882776e-04 -2.2254633e-03  1.0823170e-04\n",
      "  8.5038779e-04  1.4318284e-03 -4.9882889e-04  2.9524644e-03\n",
      "  9.1584760e-04 -1.4159635e-03 -1.9208459e-03 -2.6151182e-03\n",
      " -4.7089040e-04  2.8544508e-03 -2.6390397e-03 -2.0932956e-03\n",
      "  3.1605172e-03 -1.2894261e-03]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"bitcoin will rise again\"\n",
    "vector = model.infer_vector(simple_preprocess(test_text))\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1cfd7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_doc2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "551003f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"my_doc2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547d8dd",
   "metadata": {},
   "source": [
    "### Task 1. Practice finding similar documents/articles/posts. Assess validity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "da57a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank Distribution: Counter({0: 292, 1: 8})\n",
      "Test Document (32): ¬´at least three democrats are considering splitting from the party while no one has yet nominated to contest the leadership three of the gang of four senators who ousted natasha stott despoja from the leadership are considering forming new progressive centre party in the fallout from last week turmoil this would leave the democrats with rump of three or four members west australian senator andrew murray said yesterday unless the democrats left wing gave ground the party would split¬ª\n",
      "\n",
      "Most Similar Documents:\n",
      "Document 9736 is out of bounds for train_corpus.\n",
      "\n",
      "Document 3902 is out of bounds for train_corpus.\n",
      "\n",
      "Document 4467 is out of bounds for train_corpus.\n",
      "\n",
      "Document 2609 is out of bounds for train_corpus.\n",
      "\n",
      "Document 3229 is out of bounds for train_corpus.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze rank distribution\n",
    "print(\"Rank Distribution:\", counter)\n",
    "\n",
    "# Pick a random test document\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=5)\n",
    "\n",
    "print(f\"Test Document ({doc_id}): ¬´{' '.join(test_corpus[doc_id])}¬ª\\n\")\n",
    "\n",
    "# Display most similar documents\n",
    "print(\"Most Similar Documents:\")\n",
    "for sim_id, similarity in sims:\n",
    "    try:\n",
    "        sim_id = int(sim_id)  # Convert document ID to integer\n",
    "        if 0 <= sim_id < len(train_corpus):  \n",
    "            print(f\"Document {sim_id} (Similarity: {similarity:.4f}): ¬´{' '.join(train_corpus[sim_id].words)}¬ª\\n\")\n",
    "        else:\n",
    "            print(f\"Document {sim_id} is out of bounds for train_corpus.\\n\")\n",
    "    except (ValueError, IndexError) as e:\n",
    "        print(f\"Error processing document {sim_id}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014aee1",
   "metadata": {},
   "source": [
    "# Variant 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9f9692cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Solana is a great blockchain for NFTs\"\n",
    "query_vector = model.infer_vector(simple_preprocess(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4a8060d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Bitcoin is the future of money\n",
      "\n",
      "Top-5 Similar Documents:\n",
      "\n",
      "--- Similarity: 0.2872 ---\n",
      "The Ten Steps to Metaverse Interoperability. \n",
      "Step Six: Business Model Alignment\n",
      "@KZeroWorldswide #metaverse #Interoperability #Blockchain #Web3 #smartcontracts\n",
      "#defi #fintech @nicmitham\n",
      "\n",
      "--- Similarity: 0.2859 ---\n",
      "If you could integrate the latest version of GPT-4 or any future iterations into Worldcoin, I believe it would greatly enhance the value of both, potentially advancing the AI world even further.\n",
      "\n",
      "#saga2056 #saga #worldcoin #ai #Gamefi #openai #googleai #samaltman #NFT #eth #btc\n",
      "\n",
      "--- Similarity: 0.2763 ---\n",
      "Is the crypto bull run over? What comes next?üìâ\n",
      "\n",
      "Imagine a world where even drastic crypto dips boost your blazing confidence.\n",
      "\n",
      "Transform uncertainty into security with inSure DeFi‚Äîthe world's first decentralized #crypto insurance platform.\n",
      "\n",
      "#rwa #ai $sure #eth #insure #btc #defi\n",
      "\n",
      "--- Similarity: 0.2608 ---\n",
      "I want to analyze $DNX from a fundamentals perspective. Let‚Äôs dive in ü§ø\n",
      "\n",
      "$SOL in 2020 was worth $0.50 per coin, and after the bull run in 2021, it surged up to $250 per coin! \n",
      "\n",
      "#SOL is a #POS coin. It has smart contracts like #ETH, but with cheaper fees, and it supports #DeFi as well. In 2021, it rose 500x from its $0.50 low in 2020 üöÄ \n",
      "\n",
      "$DNX has much more tremendous potential and use cases compared to #SOL. #SOL can't solve real-world problems such as pharmaceutical drug discovery or detecting breast cancer and HIV viruses. $DNX can be used in CFD wind tunnels to optimize car aerodynamics, train large LLM models for teaching new languages to ChatGPT, enhance self-driving automotive technology for better autonomous driving, and provide the best financial advice for better decision-making üî• \n",
      "\n",
      "$DNX can save human lives because early detection of cancer can lead to a cure. If $SOL, with lower utility, can surge 500X, why can't $DNX exceed $SOL and reach $500-$1000 per coin?\n",
      "\n",
      "If you don't understand the potential of DNX yet, I don't have time to convince you! We'll meet each other at the top, in the billion-dollar market cap range üëÄ\n",
      "\n",
      "#Dynex #PoUW #Quantum #AI\n",
      "\n",
      "--- Similarity: 0.2607 ---\n",
      "This project is a good project.  There‚Äôs a lot of projects out there but this one really caught my eye at the first sight. A great &amp; legitimate project aiming for a better future.\n",
      "#Memecoin #MEME #CRUSH $SOL $Crush #BTC #presale #Cryptocurrency #Investment\n",
      "\n",
      "--- Similarity: 0.2603 ---\n",
      "#BITCOIN solves many of the problems gold can't. Like getting on a plane and taking tens of millions or billions of $$ with you wherever you want to go in the world.\n",
      "\n",
      "--- Similarity: 0.2581 ---\n",
      "üîÆ The Future of EarnmopolyThe roadmap ahead is filled with exciting milestones:Q4 2024: Launch of our mobile app üì±Q1 2025: Integration with DeFi platforms to increase utility üí∏Q2 2025: Expansion into metaverse gaming üïπÔ∏è#Earnmopoly #CryptoGaming #TokenEconomics #DeFi\n",
      "\n",
      "--- Similarity: 0.2575 ---\n",
      "LFG\n",
      "#BiokriptX\n",
      "\n",
      "#defi\n",
      "\n",
      "#Solana\n",
      "\n",
      "--- Similarity: 0.2519 ---\n",
      "üîÆ The Future of EarnmopolyThe roadmap ahead is filled with exciting milestones:Q4 2024: Launch of our mobile app üì±Q1 2025: Integration with DeFi platforms to increase utility üí∏Q2 2025: Expansion into metaverse gaming üïπÔ∏è#Earnmopoly #CryptoGaming #TokenEconomics #DeFi\n",
      "\n",
      "--- Similarity: 0.2519 ---\n",
      "Beast Meme (XBM) is like a fun magician always dynamic, always entertaining, and full of surprises! üé©‚ú® Ready to be amazed?\n",
      "\n",
      "#beastmemexbm #be–∞stmemes #mrbeast #fanclub #crypto #sol #solana #cryptocurrency #memecoin #mrbeastfanclun\n"
     ]
    }
   ],
   "source": [
    "# Define a query to find similar documents\n",
    "query = \"Bitcoin is the future of money\"\n",
    "query_vector = model.infer_vector(simple_preprocess(query))\n",
    "\n",
    "# Find the top-5 most similar documents\n",
    "similar_docs = model.dv.most_similar([query_vector], topn=10)\n",
    "\n",
    "# Display the texts of the similar documents\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop-5 Similar Documents:\")\n",
    "for doc_id, similarity in similar_docs:\n",
    "    print(f\"\\n--- Similarity: {similarity:.4f} ---\")\n",
    "    print(texts.iloc[int(doc_id)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f20cca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: blockchain and cryptocurrency\n",
      "\n",
      "Top-5 Similar Documents:\n",
      "\n",
      "--- Similarity: 0.2747 ---\n",
      "Microstrategy Is Seeking a Full-Time Lightning Network Engineer to Build a SaaS Platform\n",
      ".\n",
      "See More:.\n",
      "#bitcoin #bitcoins #bitcoinprice #bitcoinnews #bitcoinmining #BitcoinBillionaire #bitcoincash #bitcointrading\n",
      "\n",
      "--- Similarity: 0.2248 ---\n",
      "At the D.C. Blockchain Summit, Hester Peirce called out the SECs enforcement-first approach, signaling cryptos growing influence in Washington. #Crypto #Blockchain\n",
      "\n",
      "--- Similarity: 0.2083 ---\n",
      "There is a significant increase in whale activity in BTC ‚Äî data from Santiment\n",
      "\n",
      "#Crypto #CryptoNews\n",
      "\n",
      "#Bitcoin #BTC #BitcoinETF\n",
      "\n",
      "#BTCHalving #BTCHalving2024\n",
      "\n",
      "--- Similarity: 0.2055 ---\n",
      "Core Scientific, a major Bitcoin mining company in the United States, is shifting focus to artificial intelligence (AI) to address challenges from the latest Bitcoin halving.  Read more on#web30  #blockchain #news #crypto #cryptonews #defi #nft #bitcoin\n",
      "\n",
      "--- Similarity: 0.1940 ---\n",
      "Injective is lightning fast: 0.67s block time and 25,000 TPS! ‚ö°Ô∏è \n",
      "\n",
      "Experience unparalleled speed and efficiency in the blockchain space.\n",
      "\n",
      "#INJ #Crypto #Blockchain #DeFi #Speed #Innovation\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# –¢–æ–∫–µ–Ω—ñ–∑—É—î–º–æ –∑–∞–ø–∏—Ç –∑ —Ç—ñ—î—é –∂ –ø—Ä–æ—Ü–µ–¥—É—Ä–æ—é, —â–æ –π –¥–ª—è —Ç–µ–∫—Å—Ç—ñ–≤\n",
    "def preprocess_query(query):\n",
    "    return simple_preprocess(query)\n",
    "\n",
    "# –û—Ü—ñ–Ω–∫–∞ —Å—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤\n",
    "def find_similar_documents(query, model, texts, top_n=5):\n",
    "    # –û—Ç—Ä–∏–º—É—î–º–æ –≤–µ–∫—Ç–æ—Ä –∑–∞–ø–∏—Ç—É\n",
    "    query_vector = model.infer_vector(preprocess_query(query))\n",
    "    \n",
    "    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ top_n —Å—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤\n",
    "    similar_docs = model.dv.most_similar([query_vector], topn=top_n)\n",
    "    \n",
    "    results = []\n",
    "    for doc_id, similarity in similar_docs:\n",
    "        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "        document_text = texts.iloc[int(doc_id)]\n",
    "        results.append((similarity, document_text))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø–∏—Ç\n",
    "query = \"blockchain and cryptocurrency\"\n",
    "\n",
    "# –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å—Ö–æ–∂—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏\n",
    "similar_docs = find_similar_documents(query, model, texts)\n",
    "\n",
    "# –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nTop-5 Similar Documents:\")\n",
    "for similarity, doc in similar_docs:\n",
    "    print(f\"\\n--- Similarity: {similarity:.4f} ---\")\n",
    "    print(doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
